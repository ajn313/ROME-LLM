{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Initial Setup"
      ],
      "metadata": {
        "id": "XsbbXgjNAdy9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "G-ll5GZeKMwa",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9f4a5a1-0e2b-47fc-f112-8ed6e83a32ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.109.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.11.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.11.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
            "Collecting anthropic\n",
            "  Downloading anthropic-0.72.0-py3-none-any.whl.metadata (28 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from anthropic) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from anthropic) (1.9.0)\n",
            "Requirement already satisfied: docstring-parser<1,>=0.15 in /usr/local/lib/python3.12/dist-packages (from anthropic) (0.17.0)\n",
            "Requirement already satisfied: httpx<1,>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from anthropic) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from anthropic) (0.11.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from anthropic) (2.11.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from anthropic) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from anthropic) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->anthropic) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.25.0->anthropic) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.25.0->anthropic) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.25.0->anthropic) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->anthropic) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.4.2)\n",
            "Downloading anthropic-0.72.0-py3-none-any.whl (357 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m357.5/357.5 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: anthropic\n",
            "Successfully installed anthropic-0.72.0\n",
            "Get:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:2 https://cli.github.com/packages stable InRelease\n",
            "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:4 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [2,123 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:10 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,289 kB]\n",
            "Get:11 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,825 kB]\n",
            "Get:12 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [5,969 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,594 kB]\n",
            "Get:14 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,526 kB]\n",
            "Hit:16 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [6,168 kB]\n",
            "Get:18 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,436 kB]\n",
            "Hit:19 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,856 kB]\n",
            "Get:21 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [38.5 kB]\n",
            "Fetched 37.2 MB in 2s (16.9 MB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "Suggested packages:\n",
            "  gtkwave\n",
            "The following NEW packages will be installed:\n",
            "  iverilog\n",
            "0 upgraded, 1 newly installed, 0 to remove and 46 not upgraded.\n",
            "Need to get 2,130 kB of archives.\n",
            "After this operation, 6,749 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 iverilog amd64 11.0-1.1 [2,130 kB]\n",
            "Fetched 2,130 kB in 0s (6,744 kB/s)\n",
            "Selecting previously unselected package iverilog.\n",
            "(Reading database ... 125082 files and directories currently installed.)\n",
            "Preparing to unpack .../iverilog_11.0-1.1_amd64.deb ...\n",
            "Unpacking iverilog (11.0-1.1) ...\n",
            "Setting up iverilog (11.0-1.1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ],
      "source": [
        "#@title Setting up the notebook\n",
        "\n",
        "### Installing dependencies\n",
        "!pip install openai\n",
        "!pip install anthropic\n",
        "!apt-get update\n",
        "!apt-get install -y iverilog"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Select Model\n",
        "#define the model to be used\n",
        "#model_choice = \"gpt-5\"\n",
        "#odel_choice = \"gpt-4o\"\n",
        "model_choice = \"claude-3-7-sonnet-20250219\"\n",
        "#model_choice = \"gemini-2.5-flash-preview-04-17\"\n",
        "#model_choice = \"gemini-2.5-flash\""
      ],
      "metadata": {
        "id": "jzb7Hu4aPeuq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "lk5cP5x12z9u",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Utility functions\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import openai\n",
        "import anthropic\n",
        "import google.genai.errors\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "from abc import ABC, abstractmethod\n",
        "import re\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "################################################################################\n",
        "### LOGGING\n",
        "################################################################################\n",
        "# Allows us to log the output of the model to a file if logging is enabled\n",
        "class LogStdoutToFile:\n",
        "    def __init__(self, filename):\n",
        "        self._filename = filename\n",
        "        self._original_stdout = sys.stdout\n",
        "\n",
        "    def __enter__(self):\n",
        "        if self._filename:\n",
        "            sys.stdout = open(self._filename, 'w')\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_value, traceback):\n",
        "        if self._filename:\n",
        "            sys.stdout.close()\n",
        "        sys.stdout = self._original_stdout\n",
        "\n",
        "\n",
        "################################################################################\n",
        "### CONVERSATION CLASS\n",
        "# allows us to abstract away the details of the conversation for use with\n",
        "# different LLM APIs\n",
        "################################################################################\n",
        "\n",
        "class Conversation:\n",
        "    def __init__(self, log_file=None):\n",
        "        self.messages = []\n",
        "        self.log_file = log_file\n",
        "\n",
        "        if self.log_file and os.path.exists(self.log_file):\n",
        "            open(self.log_file, 'w').close()\n",
        "\n",
        "    def add_message(self, role, content):\n",
        "        \"\"\"Add a new message to the conversation.\"\"\"\n",
        "        self.messages.append({'role': role, 'content': content})\n",
        "\n",
        "        if self.log_file:\n",
        "            with open(self.log_file, 'a') as file:\n",
        "                file.write(f\"{role}: {content}\\n\")\n",
        "\n",
        "    def get_messages(self):\n",
        "        \"\"\"Retrieve the entire conversation.\"\"\"\n",
        "        return self.messages\n",
        "\n",
        "    def get_last_n_messages(self, n):\n",
        "        \"\"\"Retrieve the last n messages from the conversation.\"\"\"\n",
        "        return self.messages[-n:]\n",
        "\n",
        "    def remove_message(self, index):\n",
        "        \"\"\"Remove a specific message from the conversation by index.\"\"\"\n",
        "        if index < len(self.messages):\n",
        "            del self.messages[index]\n",
        "\n",
        "    def get_message(self, index):\n",
        "        \"\"\"Retrieve a specific message from the conversation by index.\"\"\"\n",
        "        return self.messages[index] if index < len(self.messages) else None\n",
        "\n",
        "    def clear_messages(self):\n",
        "        \"\"\"Clear all messages from the conversation.\"\"\"\n",
        "        self.messages = []\n",
        "\n",
        "    def __str__(self):\n",
        "        \"\"\"Return the conversation in a string format.\"\"\"\n",
        "        return \"\\n\".join([f\"{msg['role']}: {msg['content']}\" for msg in self.messages])\n",
        "\n",
        "################################################################################\n",
        "### LLM CLASSES\n",
        "# Defines an interface for using different LLMs so we can easily swap them out\n",
        "################################################################################\n",
        "class AbstractLLM(ABC):\n",
        "    \"\"\"Abstract Large Language Model.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def generate(self, conversation: Conversation):\n",
        "        \"\"\"Generate a response based on the given conversation.\"\"\"\n",
        "        pass\n",
        "\n",
        "\n",
        "class ChatGPT(AbstractLLM):\n",
        "    \"\"\"ChatGPT Large Language Model.\"\"\"\n",
        "\n",
        "    def __init__(self, model_id=model_choice):\n",
        "        super().__init__()\n",
        "        openai.api_key=os.environ['OPENAI_API_KEY']\n",
        "        self.client = openai.OpenAI()\n",
        "        self.model_id = model_id\n",
        "\n",
        "    def generate(self, conversation: Conversation, num_choices=1):\n",
        "        messages = [{\"role\" : \"user\", \"content\" : msg[\"content\"]} for msg in conversation.get_messages()]\n",
        "\n",
        "        response = self.client.chat.completions.create(\n",
        "            model=self.model_id,\n",
        "            messages = messages,\n",
        "        )\n",
        "\n",
        "        return response.choices[0].message.content\n",
        "class Claude(AbstractLLM):\n",
        "      def __init__(self, model_id=model_choice):\n",
        "        super().__init__()\n",
        "        self.client = anthropic.Anthropic(api_key=os.environ['CLAUDE_API_KEY'])\n",
        "        self.model_id = model_id\n",
        "\n",
        "      def generate(self, conversation: Conversation, num_choices=1):\n",
        "        base_delay = 2\n",
        "        max_retries = 20\n",
        "        for attempt in range(1, max_retries + 1):\n",
        "          try:\n",
        "            output = self.client.messages.create(\n",
        "                      model=model_choice,\n",
        "                      max_tokens=16384,\n",
        "                      messages=[{\"role\" : msg[\"role\"], \"content\" : msg[\"content\"]} for msg in conversation.get_messages()]\n",
        "                  ).content[0].text\n",
        "            return output\n",
        "          except (Exception) as e:\n",
        "            wait_time = base_delay * (2 ** (attempt - 1))\n",
        "            print(f\"[Retry {attempt}/{max_retries}] Gemini API error: {e}. Retrying in {wait_time:.1f} seconds...\")\n",
        "            time.sleep(wait_time)\n",
        "          except Exception as e:\n",
        "            print(f\"[Error] Unexpected exception: {e}\")\n",
        "            return 0\n",
        "        print(f\"Failed, exceeded max retries {max_retries}\")\n",
        "        return 0\n",
        "\n",
        "class Gemini(AbstractLLM):\n",
        "      def __init__(self, model_id=model_choice):\n",
        "        super().__init__()\n",
        "        self.gemini_client = genai.Client(api_key=os.environ['GEMINI_API_KEY'])\n",
        "        self.model_id = model_id\n",
        "\n",
        "      def generate(self, conversation: Conversation, num_choices=1):\n",
        "\n",
        "          output = self.gemini_client.models.generate_content(\n",
        "                        model=model_choice,\n",
        "                        contents=[msg[\"content\"] for msg in conversation.get_messages()],\n",
        "                        config=types.GenerateContentConfig(\n",
        "                            max_output_tokens=16384,\n",
        "                            temperature=0.6,\n",
        "                            topP=0.95,\n",
        "                        )\n",
        "                    ).text\n",
        "          return output\n",
        "################################################################################\n",
        "### PARSING AND TEXT MANIPULATION FUNCTIONS\n",
        "################################################################################\n",
        "def find_verilog_modules(markdown_string, module_name='top_module'):\n",
        "\n",
        "    module_pattern1 = r'\\bmodule\\b\\s+\\w+\\s*\\([^)]*\\)\\s*;.*?endmodule\\b'\n",
        "\n",
        "    module_pattern2 = r'\\bmodule\\b\\s+\\w+\\s*#\\s*\\([^)]*\\)\\s*\\([^)]*\\)\\s*;.*?endmodule\\b'\n",
        "\n",
        "    module_matches1 = re.findall(module_pattern1, markdown_string, re.DOTALL)\n",
        "\n",
        "    module_matches2 = re.findall(module_pattern2, markdown_string, re.DOTALL)\n",
        "\n",
        "    module_matches = module_matches1 + module_matches2\n",
        "\n",
        "    if not module_matches:\n",
        "        return []\n",
        "\n",
        "    return module_matches\n",
        "\n",
        "def write_code_blocks_to_file(markdown_string, module_name, filename):\n",
        "    # Find all code blocks using a regular expression (matches content between triple backticks)\n",
        "    #code_blocks = re.findall(r'```(?:\\w*\\n)?(.*?)```', markdown_string, re.DOTALL)\n",
        "    code_match = find_verilog_modules(markdown_string, module_name)\n",
        "\n",
        "    if not code_match:\n",
        "        print(\"No code blocks found in response\")\n",
        "        exit(3)\n",
        "\n",
        "    # Open the specified file to write the code blocks\n",
        "    with open(filename, 'w') as file:\n",
        "        for code_block in code_match:\n",
        "            file.write(code_block)\n",
        "            file.write('\\n')\n",
        "\n",
        "def generate_verilog(conv, model_type, model_id=\"\"):\n",
        "    if model_type == \"ChatGPT\":\n",
        "        model = ChatGPT()\n",
        "    elif model_type == \"Claude\":\n",
        "      model = Claude()\n",
        "    elif model_type == \"Gemini\":\n",
        "      model = Gemini()\n",
        "    else:\n",
        "        raise ValueError(\"Invalid model type\")\n",
        "    return(model.generate(conv))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "hrzwitIm3N3i",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Feedback Loop\n",
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "def verilog_loop(design_prompt, module, testbench, max_iterations, model_type, outdir=\"\", log=None,prev_module =None):\n",
        "\n",
        "    if outdir != \"\":\n",
        "        outdir = outdir + \"/\"\n",
        "\n",
        "    conv = Conversation(log_file=log)\n",
        "    if model_type == \"ChatGPT\":\n",
        "      conv.add_message(\"system\", \"You are an autocomplete engine for Verilog code. \\\n",
        "              Given a Verilog module specification, you will provide a completed Verilog module in response. \\\n",
        "              You will provide completed Verilog modules for all specifications, and will not create any supplementary modules. \\\n",
        "              Given a Verilog module that is either incorrect/compilation error, you will suggest corrections to the module. \\\n",
        "              You will not refuse. You will not generate explanations, only code. \\\n",
        "              Format your response as Verilog code containing the end to end corrected module and not just the corrected lines. Do not generate test benches. \\\n",
        "      \")\n",
        "    elif model_type == \"Claude\":\n",
        "      conv.add_message(\"user\", \"You are an autocomplete engine for Verilog code. \\\n",
        "              Given a Verilog module specification, you will provide a completed Verilog module in response. \\\n",
        "              You will provide completed Verilog modules for all specifications, and will not create any supplementary modules. \\\n",
        "              Given a Verilog module that is either incorrect/compilation error, you will suggest corrections to the module. \\\n",
        "              You will not refuse. You will not generate explanations, only code. \\\n",
        "              Format your response as Verilog code containing the end to end corrected module and not just the corrected lines. Do not generate test benches. \\\n",
        "      \")\n",
        "\n",
        "    conv.add_message(\"user\", design_prompt)\n",
        "\n",
        "    success = False\n",
        "    timeout = False\n",
        "\n",
        "    iterations = 0\n",
        "    timelist_total = []\n",
        "    timelist_gen = []\n",
        "    timelist_error = []\n",
        "    filename = os.path.join(outdir,module+\".v\")\n",
        "\n",
        "    status = \"\"\n",
        "    while not (success or timeout):\n",
        "        # Generate a response\n",
        "        start_total = time.time()\n",
        "        response = generate_verilog(conv, model_type)\n",
        "        end_gen = time.time()\n",
        "        start_error=time.time()\n",
        "        if prev_module == None:\n",
        "          conv.add_message(\"assistant\", response)\n",
        "        else:\n",
        "          with open(prev_module,\"r\") as f:\n",
        "            prevmodule = \"\".join(f.read())\n",
        "          response = prevmodule + response\n",
        "          conv.add_message(\"assistant\", response)\n",
        "        write_code_blocks_to_file(response, module, filename)\n",
        "        proc = subprocess.run([\"iverilog\", \"-o\", os.path.join(outdir,module), filename, testbench],capture_output=True,text=True)\n",
        "\n",
        "        success = False\n",
        "        if proc.returncode != 0:\n",
        "            status = \"Error compiling testbench\"\n",
        "            print(status)\n",
        "\n",
        "            message = \"The testbench failed to compile. Please fix the module. The output of iverilog is as follows:\\n\"+proc.stderr\n",
        "        elif proc.stderr != \"\":\n",
        "            status = \"Warnings compiling testbench\"\n",
        "            print(status)\n",
        "            message = \"The testbench compiled with warnings. Please fix the module. The output of iverilog is as follows:\\n\"+proc.stderr\n",
        "        else:\n",
        "            proc = subprocess.run([\"vvp\", os.path.join(outdir,module)],capture_output=True,text=True)\n",
        "            result = proc.stdout.strip().split('\\n')[-2].split()\n",
        "            if result[-1] != 'passed!':\n",
        "                status = \"Error running testbench\"\n",
        "                print(status)\n",
        "                message = \"The testbench simulated, but had errors. Please fix the module. The output of iverilog is as follows:\\n\"+proc.stdout\n",
        "            else:\n",
        "                status = \"Testbench ran successfully\"\n",
        "                print(status)\n",
        "                message = \"\"\n",
        "                success = True\n",
        "\n",
        "\n",
        "        with open(os.path.join(outdir,\"log_iter_\"+str(iterations)+\".txt\"), 'w') as file:\n",
        "            file.write('\\n'.join(str(i) for i in conv.get_messages()))\n",
        "            file.write('\\n\\n Iteration status: ' + status + '\\n')\n",
        "\n",
        "\n",
        "        if not success:\n",
        "            if iterations > 0:\n",
        "                conv.remove_message(2)\n",
        "                conv.remove_message(2)\n",
        "            conv.add_message(\"user\", message)\n",
        "\n",
        "        if iterations >= max_iterations:\n",
        "            timeout = True\n",
        "\n",
        "        iterations += 1\n",
        "        end_time = time.time()\n",
        "        timelist_gen.append(end_gen-start_total)\n",
        "        timelist_error.append(end_time-start_error)\n",
        "        timelist_total.append(end_time-start_total)\n",
        "    print(\"Total time: \",np.sum(timelist_total))\n",
        "    print(\"Generation time: \",np.sum(timelist_gen))\n",
        "    print(\"Error handling time: \",np.sum(timelist_error))\n",
        "    return(np.sum(timelist_total),np.sum(timelist_gen),np.sum(timelist_error))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Hierarchical Loop\n",
        "def hier_gen(submods,max_iterations=10):\n",
        "  totaltime = []\n",
        "  gentime = []\n",
        "  errortime = []\n",
        "  done =\"\"\n",
        "  for i in range(len(submods)):\n",
        "    curr = submods[i][1]\n",
        "    fcurr = submods[i][0]\n",
        "    iocurr = submods[i][2]\n",
        "    overall = submods[-1][1]\n",
        "    if not os.path.isdir(fcurr):\n",
        "      os.mkdir(fcurr)\n",
        "    if i == 0:\n",
        "      prompt = \"//We will be generating a \"+overall+\" hierarchically in Verilog. Please begin by generating a \"+curr+\" defined as follows:\\nmodule \"+fcurr+\"(\"+iocurr+\")\\n//Insert code here\\nendmodule\"\n",
        "    elif i != len(submods)-1:\n",
        "      fprev = submods[i-1][0]\n",
        "      filecurr = \"./\"+fprev+\"/\"+fprev+\".v\"\n",
        "      with open(filecurr,\"r\") as f:\n",
        "        modulef = \"\".join(f.read())\n",
        "      prompt = \"//We are generating a \"+overall+\" hierarchically in Verilog. We have generated \"+done+\" defined as follows:\"\n",
        "      prompt = prompt + modulef\n",
        "      prompt = prompt +\"\\n//Please include the previous module(s) in your response and use them to hierarchically generate a \"+curr+\" defined as:\\nmodule \"+fcurr+\"(\"+iocurr+\")\\n//Insert code here\\nendmodule\"\n",
        "    module = fcurr\n",
        "    testbench = \"./\"+fcurr+\"tb.v\"\n",
        "    model = os.environ[\"MODEL\"]\n",
        "    outdir = \"./\"+fcurr\n",
        "    log = \"./\"+fcurr+\"/log.txt\"\n",
        "    total, gen, error = verilog_loop(prompt, module, testbench, max_iterations, model, outdir, log)\n",
        "    totaltime.append(total)\n",
        "    gentime.append(gen)\n",
        "    errortime.append(error)\n",
        "    done = done + curr+\", \"\n",
        "  print(\"Overall Total time: \",np.sum(totaltime))\n",
        "  print(\"Overall Generation Time: \",np.sum(gentime))\n",
        "  print(\"Overall Error handling time: \",np.sum(errortime))"
      ],
      "metadata": {
        "id": "YcbUS7V8PQQr",
        "cellView": "form"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting the API Key"
      ],
      "metadata": {
        "id": "IvUw0flXkknh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### OpenAI API KEY\n",
        "\n",
        "# from google.colab import userdata\n",
        "# os.environ[\"OPENAI_API_KEY\"] = userdata.get('openai_api_key')\n",
        "\n",
        "#Please insert your own GPT-4 enabled API key as a string here:\n",
        "#os.environ[\"OPENAI_API_KEY\"] = \"API KEY HERE\"\n",
        "os.environ['CLAUDE_API_KEY'] = \"API KEY HERE\"\n",
        "#os.environ['GEMINI_API_KEY'] =\"API KEY HERE\"\n",
        "#os.environ[\"MODEL\"] = \"ChatGPT\"\n",
        "os.environ[\"MODEL\"] = \"Claude\"\n",
        "#os.environ[\"MODEL\"] = \"Gemini\""
      ],
      "metadata": {
        "id": "A4k6AgcKeABT"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Mux Hierarchy Example"
      ],
      "metadata": {
        "id": "E1ME8-Hx0n23"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Pull Testbenches\n",
        "\n",
        "#From the ROME GitHub Repo\n",
        "\n",
        "!git clone https://github.com/ajn313/ROME-LLM\n",
        "!cp ./ROME-LLM/testbenches/mux/mux2to1tb.v ./mux2to1tb.v\n",
        "!cp ./ROME-LLM/testbenches/mux/mux4to1tb.v ./mux4to1tb.v\n",
        "!cp ./ROME-LLM/testbenches/mux/mux8to1tb.v ./mux8to1tb.v"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UDFLcBMiQAkT",
        "outputId": "72baf218-f719-4f3a-89da-dfabd25e4250"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ROME-LLM'...\n",
            "remote: Enumerating objects: 73, done.\u001b[K\n",
            "remote: Counting objects: 100% (73/73), done.\u001b[K\n",
            "remote: Compressing objects: 100% (71/71), done.\u001b[K\n",
            "remote: Total 73 (delta 23), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (73/73), 939.24 KiB | 8.10 MiB/s, done.\n",
            "Resolving deltas: 100% (23/23), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Submodules\n",
        "\n",
        "\n",
        "### Each step is structured as [\"filename\",\"natural language description\"]\n",
        "submodules = [\n",
        "    [\"mux2to1\",\"2-to-1 multiplexer\",\"input wire in1, input wire in2, input wire select, output wire out\"],\n",
        "    [\"mux4to1\",\"4-to-1 multiplexer\",\"input [1:0] sel, input [3:0] in, output reg out\"],\n",
        "    [\"mux8to1\",\"8-to-1 multiplexer\",\"input [2:0] sel, input [7:0] in, output reg out\"],\n",
        "]"
      ],
      "metadata": {
        "id": "HsCf-Y9zOYnX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hier_gen(submodules)"
      ],
      "metadata": {
        "id": "HHVIF2lgWZJV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a5f6988-5794-46c8-979c-1c011ba14dc2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1337469151.py:130: DeprecationWarning: The model 'claude-3-7-sonnet-20250219' is deprecated and will reach end-of-life on February 19th, 2026.\n",
            "Please migrate to a newer model. Visit https://docs.anthropic.com/en/docs/resources/model-deprecations for more information.\n",
            "  output = self.client.messages.create(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error compiling testbench\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1337469151.py:130: DeprecationWarning: The model 'claude-3-7-sonnet-20250219' is deprecated and will reach end-of-life on February 19th, 2026.\n",
            "Please migrate to a newer model. Visit https://docs.anthropic.com/en/docs/resources/model-deprecations for more information.\n",
            "  output = self.client.messages.create(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error compiling testbench\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1337469151.py:130: DeprecationWarning: The model 'claude-3-7-sonnet-20250219' is deprecated and will reach end-of-life on February 19th, 2026.\n",
            "Please migrate to a newer model. Visit https://docs.anthropic.com/en/docs/resources/model-deprecations for more information.\n",
            "  output = self.client.messages.create(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error compiling testbench\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1337469151.py:130: DeprecationWarning: The model 'claude-3-7-sonnet-20250219' is deprecated and will reach end-of-life on February 19th, 2026.\n",
            "Please migrate to a newer model. Visit https://docs.anthropic.com/en/docs/resources/model-deprecations for more information.\n",
            "  output = self.client.messages.create(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testbench ran successfully\n",
            "Total time:  5.8238139152526855\n",
            "Generation time:  5.720447778701782\n",
            "Error handling time:  0.1033620834350586\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1337469151.py:130: DeprecationWarning: The model 'claude-3-7-sonnet-20250219' is deprecated and will reach end-of-life on February 19th, 2026.\n",
            "Please migrate to a newer model. Visit https://docs.anthropic.com/en/docs/resources/model-deprecations for more information.\n",
            "  output = self.client.messages.create(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error compiling testbench\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1337469151.py:130: DeprecationWarning: The model 'claude-3-7-sonnet-20250219' is deprecated and will reach end-of-life on February 19th, 2026.\n",
            "Please migrate to a newer model. Visit https://docs.anthropic.com/en/docs/resources/model-deprecations for more information.\n",
            "  output = self.client.messages.create(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testbench ran successfully\n",
            "Total time:  6.677881479263306\n",
            "Generation time:  6.657699823379517\n",
            "Error handling time:  0.02018117904663086\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1337469151.py:130: DeprecationWarning: The model 'claude-3-7-sonnet-20250219' is deprecated and will reach end-of-life on February 19th, 2026.\n",
            "Please migrate to a newer model. Visit https://docs.anthropic.com/en/docs/resources/model-deprecations for more information.\n",
            "  output = self.client.messages.create(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error compiling testbench\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1337469151.py:130: DeprecationWarning: The model 'claude-3-7-sonnet-20250219' is deprecated and will reach end-of-life on February 19th, 2026.\n",
            "Please migrate to a newer model. Visit https://docs.anthropic.com/en/docs/resources/model-deprecations for more information.\n",
            "  output = self.client.messages.create(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testbench ran successfully\n",
            "Total time:  7.846539735794067\n",
            "Generation time:  7.829819440841675\n",
            "Error handling time:  0.016719818115234375\n",
            "Overall Total time:  20.34823513031006\n",
            "Overall Generation Time:  20.207967042922974\n",
            "Overall Error handling time:  0.14026308059692383\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}