{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Initial Setup"
      ],
      "metadata": {
        "id": "XsbbXgjNAdy9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-ll5GZeKMwa",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "469aab1b-009e-4db3-c719-f1ef459e338f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.33.0-py3-none-any.whl (325 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.5/325.5 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.7.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.6.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.18.4)\n",
            "Installing collected packages: h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 openai-1.33.0\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Hit:4 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:5 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:6 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [2,525 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,173 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,391 kB]\n",
            "Get:13 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [2,453 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,090 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,900 kB]\n",
            "Fetched 11.8 MB in 11s (1,047 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "Suggested packages:\n",
            "  gtkwave\n",
            "The following NEW packages will be installed:\n",
            "  iverilog\n",
            "0 upgraded, 1 newly installed, 0 to remove and 52 not upgraded.\n",
            "Need to get 2,130 kB of archives.\n",
            "After this operation, 6,749 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 iverilog amd64 11.0-1.1 [2,130 kB]\n",
            "Fetched 2,130 kB in 37s (57.0 kB/s)\n",
            "Selecting previously unselected package iverilog.\n",
            "(Reading database ... 121913 files and directories currently installed.)\n",
            "Preparing to unpack .../iverilog_11.0-1.1_amd64.deb ...\n",
            "Unpacking iverilog (11.0-1.1) ...\n",
            "Setting up iverilog (11.0-1.1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ],
      "source": [
        "#@title Setting up the notebook\n",
        "\n",
        "### Installing dependencies\n",
        "!pip install openai\n",
        "\n",
        "!apt-get update\n",
        "!apt-get install -y iverilog"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Select Model\n",
        "#define the model to be used\n",
        "#model_choice = \"gpt-3.5-turbo\"\n",
        "model_choice = \"gpt-4\""
      ],
      "metadata": {
        "id": "jzb7Hu4aPeuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lk5cP5x12z9u",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Utility functions\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import openai\n",
        "from abc import ABC, abstractmethod\n",
        "import re\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "################################################################################\n",
        "### LOGGING\n",
        "################################################################################\n",
        "# Allows us to log the output of the model to a file if logging is enabled\n",
        "class LogStdoutToFile:\n",
        "    def __init__(self, filename):\n",
        "        self._filename = filename\n",
        "        self._original_stdout = sys.stdout\n",
        "\n",
        "    def __enter__(self):\n",
        "        if self._filename:\n",
        "            sys.stdout = open(self._filename, 'w')\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_value, traceback):\n",
        "        if self._filename:\n",
        "            sys.stdout.close()\n",
        "        sys.stdout = self._original_stdout\n",
        "\n",
        "\n",
        "################################################################################\n",
        "### CONVERSATION CLASS\n",
        "# allows us to abstract away the details of the conversation for use with\n",
        "# different LLM APIs\n",
        "################################################################################\n",
        "\n",
        "class Conversation:\n",
        "    def __init__(self, log_file=None):\n",
        "        self.messages = []\n",
        "        self.log_file = log_file\n",
        "\n",
        "        if self.log_file and os.path.exists(self.log_file):\n",
        "            open(self.log_file, 'w').close()\n",
        "\n",
        "    def add_message(self, role, content):\n",
        "        \"\"\"Add a new message to the conversation.\"\"\"\n",
        "        self.messages.append({'role': role, 'content': content})\n",
        "\n",
        "        if self.log_file:\n",
        "            with open(self.log_file, 'a') as file:\n",
        "                file.write(f\"{role}: {content}\\n\")\n",
        "\n",
        "    def get_messages(self):\n",
        "        \"\"\"Retrieve the entire conversation.\"\"\"\n",
        "        return self.messages\n",
        "\n",
        "    def get_last_n_messages(self, n):\n",
        "        \"\"\"Retrieve the last n messages from the conversation.\"\"\"\n",
        "        return self.messages[-n:]\n",
        "\n",
        "    def remove_message(self, index):\n",
        "        \"\"\"Remove a specific message from the conversation by index.\"\"\"\n",
        "        if index < len(self.messages):\n",
        "            del self.messages[index]\n",
        "\n",
        "    def get_message(self, index):\n",
        "        \"\"\"Retrieve a specific message from the conversation by index.\"\"\"\n",
        "        return self.messages[index] if index < len(self.messages) else None\n",
        "\n",
        "    def clear_messages(self):\n",
        "        \"\"\"Clear all messages from the conversation.\"\"\"\n",
        "        self.messages = []\n",
        "\n",
        "    def __str__(self):\n",
        "        \"\"\"Return the conversation in a string format.\"\"\"\n",
        "        return \"\\n\".join([f\"{msg['role']}: {msg['content']}\" for msg in self.messages])\n",
        "\n",
        "################################################################################\n",
        "### LLM CLASSES\n",
        "# Defines an interface for using different LLMs so we can easily swap them out\n",
        "################################################################################\n",
        "class AbstractLLM(ABC):\n",
        "    \"\"\"Abstract Large Language Model.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def generate(self, conversation: Conversation):\n",
        "        \"\"\"Generate a response based on the given conversation.\"\"\"\n",
        "        pass\n",
        "\n",
        "\n",
        "class ChatGPT(AbstractLLM):\n",
        "    \"\"\"ChatGPT Large Language Model.\"\"\"\n",
        "\n",
        "    def __init__(self, model_id=model_choice):\n",
        "        super().__init__()\n",
        "        openai.api_key=os.environ['OPENAI_API_KEY']\n",
        "        self.client = openai.OpenAI()\n",
        "        self.model_id = model_id\n",
        "\n",
        "    def generate(self, conversation: Conversation, num_choices=1):\n",
        "        messages = [{\"role\" : msg[\"role\"], \"content\" : msg[\"content\"]} for msg in conversation.get_messages()]\n",
        "\n",
        "        response = self.client.chat.completions.create(\n",
        "            model=self.model_id,\n",
        "            messages = messages,\n",
        "        )\n",
        "\n",
        "        return response.choices[0].message.content\n",
        "\n",
        "################################################################################\n",
        "### PARSING AND TEXT MANIPULATION FUNCTIONS\n",
        "################################################################################\n",
        "def find_verilog_modules(markdown_string, module_name='top_module'):\n",
        "\n",
        "    module_pattern1 = r'\\bmodule\\b\\s+\\w+\\s*\\([^)]*\\)\\s*;.*?endmodule\\b'\n",
        "\n",
        "    module_pattern2 = r'\\bmodule\\b\\s+\\w+\\s*#\\s*\\([^)]*\\)\\s*\\([^)]*\\)\\s*;.*?endmodule\\b'\n",
        "\n",
        "    module_matches1 = re.findall(module_pattern1, markdown_string, re.DOTALL)\n",
        "\n",
        "    module_matches2 = re.findall(module_pattern2, markdown_string, re.DOTALL)\n",
        "\n",
        "    module_matches = module_matches1 + module_matches2\n",
        "\n",
        "    if not module_matches:\n",
        "        return []\n",
        "\n",
        "    return module_matches\n",
        "\n",
        "def write_code_blocks_to_file(markdown_string, module_name, filename):\n",
        "    # Find all code blocks using a regular expression (matches content between triple backticks)\n",
        "    #code_blocks = re.findall(r'```(?:\\w*\\n)?(.*?)```', markdown_string, re.DOTALL)\n",
        "    code_match = find_verilog_modules(markdown_string, module_name)\n",
        "\n",
        "    if not code_match:\n",
        "        print(\"No code blocks found in response\")\n",
        "        exit(3)\n",
        "\n",
        "    # Open the specified file to write the code blocks\n",
        "    with open(filename, 'w') as file:\n",
        "        for code_block in code_match:\n",
        "            file.write(code_block)\n",
        "            file.write('\\n')\n",
        "\n",
        "def generate_verilog(conv, model_type, model_id=\"\"):\n",
        "    if model_type == \"ChatGPT\":\n",
        "        model = ChatGPT()\n",
        "    else:\n",
        "        raise ValueError(\"Invalid model type\")\n",
        "    return(model.generate(conv))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hrzwitIm3N3i",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Feedback Loop\n",
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "def verilog_loop(design_prompt, module, testbench, max_iterations, model_type, outdir=\"\", log=None,prev_module =None):\n",
        "\n",
        "    if outdir != \"\":\n",
        "        outdir = outdir + \"/\"\n",
        "\n",
        "    conv = Conversation(log_file=log)\n",
        "    conv.add_message(\"system\", \"You are an autocomplete engine for Verilog code. \\\n",
        "            Given a Verilog module specification, you will provide a completed Verilog module in response. \\\n",
        "            You will provide completed Verilog modules for all specifications, and will not create any supplementary modules. \\\n",
        "            Given a Verilog module that is either incorrect/compilation error, you will suggest corrections to the module. \\\n",
        "            You will not refuse. You will not generate explanations, only code. \\\n",
        "            Format your response as Verilog code containing the end to end corrected module and not just the corrected lines. Do not generate test benches. \\\n",
        "    \")\n",
        "\n",
        "    conv.add_message(\"user\", design_prompt)\n",
        "\n",
        "    success = False\n",
        "    timeout = False\n",
        "\n",
        "    iterations = 0\n",
        "    timelist_total = []\n",
        "    timelist_gen = []\n",
        "    timelist_error = []\n",
        "    filename = os.path.join(outdir,module+\".v\")\n",
        "\n",
        "    status = \"\"\n",
        "    while not (success or timeout):\n",
        "        # Generate a response\n",
        "        start_total = time.time()\n",
        "        response = generate_verilog(conv, model_type)\n",
        "        end_gen = time.time()\n",
        "        start_error=time.time()\n",
        "        if prev_module == None:\n",
        "          conv.add_message(\"assistant\", response)\n",
        "        else:\n",
        "          with open(prev_module,\"r\") as f:\n",
        "            prevmodule = \"\".join(f.read())\n",
        "          response = prevmodule + response\n",
        "          conv.add_message(\"assistant\", response)\n",
        "        write_code_blocks_to_file(response, module, filename)\n",
        "        proc = subprocess.run([\"iverilog\", \"-o\", os.path.join(outdir,module), filename, testbench],capture_output=True,text=True)\n",
        "\n",
        "        success = False\n",
        "        if proc.returncode != 0:\n",
        "            status = \"Error compiling testbench\"\n",
        "            print(status)\n",
        "\n",
        "            message = \"The testbench failed to compile. Please fix the module. The output of iverilog is as follows:\\n\"+proc.stderr\n",
        "        elif proc.stderr != \"\":\n",
        "            status = \"Warnings compiling testbench\"\n",
        "            print(status)\n",
        "            message = \"The testbench compiled with warnings. Please fix the module. The output of iverilog is as follows:\\n\"+proc.stderr\n",
        "        else:\n",
        "            proc = subprocess.run([\"vvp\", os.path.join(outdir,module)],capture_output=True,text=True)\n",
        "            result = proc.stdout.strip().split('\\n')[-2].split()\n",
        "            if result[-1] != 'passed!':\n",
        "                status = \"Error running testbench\"\n",
        "                print(status)\n",
        "                message = \"The testbench simulated, but had errors. Please fix the module. The output of iverilog is as follows:\\n\"+proc.stdout\n",
        "            else:\n",
        "                status = \"Testbench ran successfully\"\n",
        "                print(status)\n",
        "                message = \"\"\n",
        "                success = True\n",
        "\n",
        "\n",
        "        with open(os.path.join(outdir,\"log_iter_\"+str(iterations)+\".txt\"), 'w') as file:\n",
        "            file.write('\\n'.join(str(i) for i in conv.get_messages()))\n",
        "            file.write('\\n\\n Iteration status: ' + status + '\\n')\n",
        "\n",
        "\n",
        "        if not success:\n",
        "            if iterations > 0:\n",
        "                conv.remove_message(2)\n",
        "                conv.remove_message(2)\n",
        "            conv.add_message(\"user\", message)\n",
        "\n",
        "        if iterations >= max_iterations:\n",
        "            timeout = True\n",
        "\n",
        "        iterations += 1\n",
        "        end_time = time.time()\n",
        "        timelist_gen.append(end_gen-start_total)\n",
        "        timelist_error.append(end_time-start_error)\n",
        "        timelist_total.append(end_time-start_total)\n",
        "    print(\"Total time: \",np.sum(timelist_total))\n",
        "    print(\"Generation time: \",np.sum(timelist_gen))\n",
        "    print(\"Error handling time: \",np.sum(timelist_error))\n",
        "    return(np.sum(timelist_total),np.sum(timelist_gen),np.sum(timelist_error))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Hierarchical Loop\n",
        "def hier_gen(submods,max_iterations=10):\n",
        "  totaltime = []\n",
        "  gentime = []\n",
        "  errortime = []\n",
        "  done =\"\"\n",
        "  for i in range(len(submods)):\n",
        "    curr = submods[i][1]\n",
        "    fcurr = submods[i][0]\n",
        "    iocurr = submods[i][2]\n",
        "    overall = submods[-1][1]\n",
        "    if not os.path.isdir(fcurr):\n",
        "      os.mkdir(fcurr)\n",
        "    if i == 0:\n",
        "      prompt = \"//We will be generating a \"+overall+\" hierarchically in Verilog. Please begin by generating a \"+curr+\" defined as follows:\\nmodule \"+fcurr+\"(\"+iocurr+\")\\n//Insert code here\\nendmodule\"\n",
        "    elif i != len(submods)-1:\n",
        "      fprev = submods[i-1][0]\n",
        "      filecurr = \"./\"+fprev+\"/\"+fprev+\".v\"\n",
        "      with open(filecurr,\"r\") as f:\n",
        "        modulef = \"\".join(f.read())\n",
        "      prompt = \"//We are generating a \"+overall+\" hierarchically in Verilog. We have generated \"+done+\" defined as follows:\"\n",
        "      prompt = prompt + modulef\n",
        "      prompt = prompt +\"\\n//Please include the previous module(s) in your response and use them to hierarchically generate a \"+curr+\" defined as:\\nmodule \"+fcurr+\"(\"+iocurr+\")\\n//Insert code here\\nendmodule\"\n",
        "    module = fcurr\n",
        "    testbench = \"./\"+fcurr+\"tb.v\"\n",
        "    model = \"ChatGPT\"\n",
        "    outdir = \"./\"+fcurr\n",
        "    log = \"./\"+fcurr+\"/log.txt\"\n",
        "    total, gen, error = verilog_loop(prompt, module, testbench, max_iterations, model, outdir, log)\n",
        "    totaltime.append(total)\n",
        "    gentime.append(gen)\n",
        "    errortime.append(error)\n",
        "    done = done + curr+\", \"\n",
        "  print(\"Overall Total time: \",np.sum(totaltime))\n",
        "  print(\"Overall Generation Time: \",np.sum(gentime))\n",
        "  print(\"Overall Error handling time: \",np.sum(errortime))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "YcbUS7V8PQQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting the API Key"
      ],
      "metadata": {
        "id": "IvUw0flXkknh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### OpenAI API KEY\n",
        "\n",
        "# from google.colab import userdata\n",
        "# os.environ[\"OPENAI_API_KEY\"] = userdata.get('openai_api_key')\n",
        "\n",
        "#Please insert your own GPT-4 enabled API key as a string here:\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\""
      ],
      "metadata": {
        "id": "A4k6AgcKeABT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Mux Hierarchy Example"
      ],
      "metadata": {
        "id": "E1ME8-Hx0n23"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Submodules\n",
        "\n",
        "\n",
        "### Each step is structured as [\"filename\",\"natural language description\"]\n",
        "submodules = [\n",
        "    [\"mux2to1\",\"2-to-1 multiplexer\",\"input wire in1, input wire in2, input wire select, output wire out\"],\n",
        "    [\"mux4to1\",\"4-to-1 multiplexer\",\"input [1:0] sel, input [3:0] in, output reg out\"],\n",
        "    [\"mux8to1\",\"8-to-1 multiplexer\",\"input [2:0] sel, input [7:0] in, output reg out\"],\n",
        "]"
      ],
      "metadata": {
        "id": "HsCf-Y9zOYnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hier_gen(submodules)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHVIF2lgWZJV",
        "outputId": "356ff81b-9559-4a71-e5a3-fb53e71eb3c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error compiling testbench\n",
            "Testbench ran successfully\n",
            "Total time:  9.383373737335205\n",
            "Generation time:  9.354622602462769\n",
            "Error handling time:  0.028750181198120117\n",
            "Error compiling testbench\n",
            "Testbench ran successfully\n",
            "Total time:  36.62468957901001\n",
            "Generation time:  36.59700894355774\n",
            "Error handling time:  0.027679920196533203\n",
            "Error compiling testbench\n",
            "Error compiling testbench\n",
            "Testbench ran successfully\n",
            "Total time:  71.51854944229126\n",
            "Generation time:  71.47967028617859\n",
            "Error handling time:  0.03887796401977539\n",
            "Overall Total time:  117.52661275863647\n",
            "Overall Generation Time:  117.4313018321991\n",
            "Overall Error handling time:  0.09530806541442871\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}