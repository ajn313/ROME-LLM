{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Initial Setup"
      ],
      "metadata": {
        "id": "XsbbXgjNAdy9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-ll5GZeKMwa",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Setting up the notebook\n",
        "\n",
        "### Installing dependencies\n",
        "!pip install openai\n",
        "!pip install anthropic\n",
        "!apt-get update\n",
        "!apt-get install -y iverilog"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Select Model\n",
        "#define the model to be used\n",
        "#model_choice = \"gpt-5\"\n",
        "model_choice = \"claude-3-7-sonnet-20250219\"\n",
        "#model_choice = \"gemini-2.5-flash-preview-04-17\"\n",
        "#model_choice = \"gemini-2.5-flash\""
      ],
      "metadata": {
        "id": "jzb7Hu4aPeuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lk5cP5x12z9u",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Utility functions\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import openai\n",
        "import anthropic\n",
        "import google.genai.errors\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "from abc import ABC, abstractmethod\n",
        "import re\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "################################################################################\n",
        "### LOGGING\n",
        "################################################################################\n",
        "# Allows us to log the output of the model to a file if logging is enabled\n",
        "class LogStdoutToFile:\n",
        "    def __init__(self, filename):\n",
        "        self._filename = filename\n",
        "        self._original_stdout = sys.stdout\n",
        "\n",
        "    def __enter__(self):\n",
        "        if self._filename:\n",
        "            sys.stdout = open(self._filename, 'w')\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_value, traceback):\n",
        "        if self._filename:\n",
        "            sys.stdout.close()\n",
        "        sys.stdout = self._original_stdout\n",
        "\n",
        "\n",
        "################################################################################\n",
        "### CONVERSATION CLASS\n",
        "# allows us to abstract away the details of the conversation for use with\n",
        "# different LLM APIs\n",
        "################################################################################\n",
        "\n",
        "class Conversation:\n",
        "    def __init__(self, log_file=None):\n",
        "        self.messages = []\n",
        "        self.log_file = log_file\n",
        "\n",
        "        if self.log_file and os.path.exists(self.log_file):\n",
        "            open(self.log_file, 'w').close()\n",
        "\n",
        "    def add_message(self, role, content):\n",
        "        \"\"\"Add a new message to the conversation.\"\"\"\n",
        "        self.messages.append({'role': role, 'content': content})\n",
        "\n",
        "        if self.log_file:\n",
        "            with open(self.log_file, 'a') as file:\n",
        "                file.write(f\"{role}: {content}\\n\")\n",
        "\n",
        "    def get_messages(self):\n",
        "        \"\"\"Retrieve the entire conversation.\"\"\"\n",
        "        return self.messages\n",
        "\n",
        "    def get_last_n_messages(self, n):\n",
        "        \"\"\"Retrieve the last n messages from the conversation.\"\"\"\n",
        "        return self.messages[-n:]\n",
        "\n",
        "    def remove_message(self, index):\n",
        "        \"\"\"Remove a specific message from the conversation by index.\"\"\"\n",
        "        if index < len(self.messages):\n",
        "            del self.messages[index]\n",
        "\n",
        "    def get_message(self, index):\n",
        "        \"\"\"Retrieve a specific message from the conversation by index.\"\"\"\n",
        "        return self.messages[index] if index < len(self.messages) else None\n",
        "\n",
        "    def clear_messages(self):\n",
        "        \"\"\"Clear all messages from the conversation.\"\"\"\n",
        "        self.messages = []\n",
        "\n",
        "    def __str__(self):\n",
        "        \"\"\"Return the conversation in a string format.\"\"\"\n",
        "        return \"\\n\".join([f\"{msg['role']}: {msg['content']}\" for msg in self.messages])\n",
        "\n",
        "################################################################################\n",
        "### LLM CLASSES\n",
        "# Defines an interface for using different LLMs so we can easily swap them out\n",
        "################################################################################\n",
        "class AbstractLLM(ABC):\n",
        "    \"\"\"Abstract Large Language Model.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def generate(self, conversation: Conversation):\n",
        "        \"\"\"Generate a response based on the given conversation.\"\"\"\n",
        "        pass\n",
        "\n",
        "\n",
        "class ChatGPT(AbstractLLM):\n",
        "    \"\"\"ChatGPT Large Language Model.\"\"\"\n",
        "\n",
        "    def __init__(self, model_id=model_choice):\n",
        "        super().__init__()\n",
        "        openai.api_key=os.environ['OPENAI_API_KEY']\n",
        "        self.client = openai.OpenAI()\n",
        "        self.model_id = model_id\n",
        "\n",
        "    def generate(self, conversation: Conversation, num_choices=1):\n",
        "        messages = [{\"role\" : \"user\", \"content\" : msg[\"content\"]} for msg in conversation.get_messages()]\n",
        "\n",
        "        response = self.client.chat.completions.create(\n",
        "            model=self.model_id,\n",
        "            messages = messages,\n",
        "        )\n",
        "\n",
        "        return response.choices[0].message.content\n",
        "class Claude(AbstractLLM):\n",
        "      def __init__(self, model_id=model_choice):\n",
        "        super().__init__()\n",
        "        self.client = anthropic.Anthropic(api_key=os.environ['CLAUDE_API_KEY'])\n",
        "        self.model_id = model_id\n",
        "\n",
        "      def generate(self, conversation: Conversation, num_choices=1):\n",
        "        base_delay = 2\n",
        "        max_retries = 20\n",
        "        for attempt in range(1, max_retries + 1):\n",
        "          try:\n",
        "            output = self.client.messages.create(\n",
        "                      model=model_choice,\n",
        "                      max_tokens=16384,\n",
        "                      messages=[{\"role\" : msg[\"role\"], \"content\" : msg[\"content\"]} for msg in conversation.get_messages()]\n",
        "                  ).content[0].text\n",
        "            return output\n",
        "          except (Exception) as e:\n",
        "            wait_time = base_delay * (2 ** (attempt - 1))\n",
        "            print(f\"[Retry {attempt}/{max_retries}] Gemini API error: {e}. Retrying in {wait_time:.1f} seconds...\")\n",
        "            time.sleep(wait_time)\n",
        "          except Exception as e:\n",
        "            print(f\"[Error] Unexpected exception: {e}\")\n",
        "            return 0\n",
        "        print(f\"Failed, exceeded max retries {max_retries}\")\n",
        "        return 0\n",
        "\n",
        "class Gemini(AbstractLLM):\n",
        "      def __init__(self, model_id=model_choice):\n",
        "        super().__init__()\n",
        "        self.gemini_client = genai.Client(api_key=os.environ['GEMINI_API_KEY'])\n",
        "        self.model_id = model_id\n",
        "\n",
        "      def generate(self, conversation: Conversation, num_choices=1):\n",
        "\n",
        "          output = self.gemini_client.models.generate_content(\n",
        "                        model=model_choice,\n",
        "                        contents=[msg[\"content\"] for msg in conversation.get_messages()],\n",
        "                        config=types.GenerateContentConfig(\n",
        "                            max_output_tokens=16384,\n",
        "                            temperature=0.6,\n",
        "                            topP=0.95,\n",
        "                        )\n",
        "                    ).text\n",
        "          return output\n",
        "################################################################################\n",
        "### PARSING AND TEXT MANIPULATION FUNCTIONS\n",
        "################################################################################\n",
        "def find_verilog_modules(markdown_string, module_name='top_module'):\n",
        "\n",
        "    module_pattern1 = r'\\bmodule\\b\\s+\\w+\\s*\\([^)]*\\)\\s*;.*?endmodule\\b'\n",
        "\n",
        "    module_pattern2 = r'\\bmodule\\b\\s+\\w+\\s*#\\s*\\([^)]*\\)\\s*\\([^)]*\\)\\s*;.*?endmodule\\b'\n",
        "\n",
        "    module_matches1 = re.findall(module_pattern1, markdown_string, re.DOTALL)\n",
        "\n",
        "    module_matches2 = re.findall(module_pattern2, markdown_string, re.DOTALL)\n",
        "\n",
        "    module_matches = module_matches1 + module_matches2\n",
        "\n",
        "    if not module_matches:\n",
        "        return []\n",
        "\n",
        "    return module_matches\n",
        "\n",
        "def write_code_blocks_to_file(markdown_string, module_name, filename):\n",
        "    # Find all code blocks using a regular expression (matches content between triple backticks)\n",
        "    #code_blocks = re.findall(r'```(?:\\w*\\n)?(.*?)```', markdown_string, re.DOTALL)\n",
        "    code_match = find_verilog_modules(markdown_string, module_name)\n",
        "\n",
        "    if not code_match:\n",
        "        print(\"No code blocks found in response\")\n",
        "        exit(3)\n",
        "\n",
        "    # Open the specified file to write the code blocks\n",
        "    with open(filename, 'w') as file:\n",
        "        for code_block in code_match:\n",
        "            file.write(code_block)\n",
        "            file.write('\\n')\n",
        "\n",
        "def generate_verilog(conv, model_type, model_id=\"\"):\n",
        "    if model_type == \"ChatGPT\":\n",
        "        model = ChatGPT()\n",
        "    elif model_type == \"Claude\":\n",
        "      model = Claude()\n",
        "    elif model_type == \"Gemini\":\n",
        "      model = Gemini()\n",
        "    else:\n",
        "        raise ValueError(\"Invalid model type\")\n",
        "    return(model.generate(conv))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hrzwitIm3N3i",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Feedback Loop\n",
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "def verilog_loop(design_prompt, module, testbench, max_iterations, model_type, outdir=\"\", log=None,prev_module =None):\n",
        "\n",
        "    if outdir != \"\":\n",
        "        outdir = outdir + \"/\"\n",
        "\n",
        "    conv = Conversation(log_file=log)\n",
        "    if model_type == \"ChatGPT\":\n",
        "      conv.add_message(\"system\", \"You are an autocomplete engine for Verilog code. \\\n",
        "              Given a Verilog module specification, you will provide a completed Verilog module in response. \\\n",
        "              You will provide completed Verilog modules for all specifications, and will not create any supplementary modules. \\\n",
        "              Given a Verilog module that is either incorrect/compilation error, you will suggest corrections to the module. \\\n",
        "              You will not refuse. You will not generate explanations, only code. \\\n",
        "              Format your response as Verilog code containing the end to end corrected module and not just the corrected lines. Do not generate test benches. \\\n",
        "      \")\n",
        "    elif model_type == \"Claude\":\n",
        "      conv.add_message(\"user\", \"You are an autocomplete engine for Verilog code. \\\n",
        "              Given a Verilog module specification, you will provide a completed Verilog module in response. \\\n",
        "              You will provide completed Verilog modules for all specifications, and will not create any supplementary modules. \\\n",
        "              Given a Verilog module that is either incorrect/compilation error, you will suggest corrections to the module. \\\n",
        "              You will not refuse. You will not generate explanations, only code. \\\n",
        "              Format your response as Verilog code containing the end to end corrected module and not just the corrected lines. Do not generate test benches. \\\n",
        "      \")\n",
        "\n",
        "    conv.add_message(\"user\", design_prompt)\n",
        "\n",
        "    success = False\n",
        "    timeout = False\n",
        "\n",
        "    iterations = 0\n",
        "    timelist_total = []\n",
        "    timelist_gen = []\n",
        "    timelist_error = []\n",
        "    filename = os.path.join(outdir,module+\".v\")\n",
        "\n",
        "    status = \"\"\n",
        "    while not (success or timeout):\n",
        "        # Generate a response\n",
        "        start_total = time.time()\n",
        "        response = generate_verilog(conv, model_type)\n",
        "        end_gen = time.time()\n",
        "        start_error=time.time()\n",
        "        if prev_module == None:\n",
        "          conv.add_message(\"assistant\", response)\n",
        "        else:\n",
        "          with open(prev_module,\"r\") as f:\n",
        "            prevmodule = \"\".join(f.read())\n",
        "          response = prevmodule + response\n",
        "          conv.add_message(\"assistant\", response)\n",
        "        write_code_blocks_to_file(response, module, filename)\n",
        "        proc = subprocess.run([\"iverilog\", \"-o\", os.path.join(outdir,module), filename, testbench],capture_output=True,text=True)\n",
        "\n",
        "        success = False\n",
        "        if proc.returncode != 0:\n",
        "            status = \"Error compiling testbench\"\n",
        "            print(status)\n",
        "\n",
        "            message = \"The testbench failed to compile. Please fix the module. The output of iverilog is as follows:\\n\"+proc.stderr\n",
        "        elif proc.stderr != \"\":\n",
        "            status = \"Warnings compiling testbench\"\n",
        "            print(status)\n",
        "            message = \"The testbench compiled with warnings. Please fix the module. The output of iverilog is as follows:\\n\"+proc.stderr\n",
        "        else:\n",
        "            proc = subprocess.run([\"vvp\", os.path.join(outdir,module)],capture_output=True,text=True)\n",
        "            result = proc.stdout.strip().split('\\n')[-2].split()\n",
        "            if result[-1] != 'passed!':\n",
        "                status = \"Error running testbench\"\n",
        "                print(status)\n",
        "                message = \"The testbench simulated, but had errors. Please fix the module. The output of iverilog is as follows:\\n\"+proc.stdout\n",
        "            else:\n",
        "                status = \"Testbench ran successfully\"\n",
        "                print(status)\n",
        "                message = \"\"\n",
        "                success = True\n",
        "\n",
        "\n",
        "        with open(os.path.join(outdir,\"log_iter_\"+str(iterations)+\".txt\"), 'w') as file:\n",
        "            file.write('\\n'.join(str(i) for i in conv.get_messages()))\n",
        "            file.write('\\n\\n Iteration status: ' + status + '\\n')\n",
        "\n",
        "\n",
        "        if not success:\n",
        "            if iterations > 0:\n",
        "                conv.remove_message(2)\n",
        "                conv.remove_message(2)\n",
        "            conv.add_message(\"user\", message)\n",
        "\n",
        "        if iterations >= max_iterations:\n",
        "            timeout = True\n",
        "\n",
        "        iterations += 1\n",
        "        end_time = time.time()\n",
        "        timelist_gen.append(end_gen-start_total)\n",
        "        timelist_error.append(end_time-start_error)\n",
        "        timelist_total.append(end_time-start_total)\n",
        "    print(\"Total time: \",np.sum(timelist_total))\n",
        "    print(\"Generation time: \",np.sum(timelist_gen))\n",
        "    print(\"Error handling time: \",np.sum(timelist_error))\n",
        "    return(np.sum(timelist_total),np.sum(timelist_gen),np.sum(timelist_error))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Hierarchical Loop\n",
        "def hier_gen(submods,max_iterations=10):\n",
        "  totaltime = []\n",
        "  gentime = []\n",
        "  errortime = []\n",
        "  done =\"\"\n",
        "  for i in range(len(submods)):\n",
        "    curr = submods[i][1]\n",
        "    fcurr = submods[i][0]\n",
        "    iocurr = submods[i][2]\n",
        "    overall = submods[-1][1]\n",
        "    if not os.path.isdir(fcurr):\n",
        "      os.mkdir(fcurr)\n",
        "    if i == 0:\n",
        "      prompt = \"//We will be generating a \"+overall+\" hierarchically in Verilog. Please begin by generating a \"+curr+\" defined as follows:\\nmodule \"+fcurr+\"(\"+iocurr+\")\\n//Insert code here\\nendmodule\"\n",
        "    elif i != len(submods)-1:\n",
        "      fprev = submods[i-1][0]\n",
        "      filecurr = \"./\"+fprev+\"/\"+fprev+\".v\"\n",
        "      with open(filecurr,\"r\") as f:\n",
        "        modulef = \"\".join(f.read())\n",
        "      prompt = \"//We are generating a \"+overall+\" hierarchically in Verilog. We have generated \"+done+\" defined as follows:\"\n",
        "      prompt = prompt + modulef\n",
        "      prompt = prompt +\"\\n//Please include the previous module(s) in your response and use them to hierarchically generate a \"+curr+\" defined as:\\nmodule \"+fcurr+\"(\"+iocurr+\")\\n//Insert code here\\nendmodule\"\n",
        "    module = fcurr\n",
        "    testbench = \"./\"+fcurr+\"tb.v\"\n",
        "    model = os.environ[\"MODEL\"]\n",
        "    outdir = \"./\"+fcurr\n",
        "    log = \"./\"+fcurr+\"/log.txt\"\n",
        "    total, gen, error = verilog_loop(prompt, module, testbench, max_iterations, model, outdir, log)\n",
        "    totaltime.append(total)\n",
        "    gentime.append(gen)\n",
        "    errortime.append(error)\n",
        "    done = done + curr+\", \"\n",
        "  print(\"Overall Total time: \",np.sum(totaltime))\n",
        "  print(\"Overall Generation Time: \",np.sum(gentime))\n",
        "  print(\"Overall Error handling time: \",np.sum(errortime))"
      ],
      "metadata": {
        "id": "YcbUS7V8PQQr",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting the API Key"
      ],
      "metadata": {
        "id": "IvUw0flXkknh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### OpenAI API KEY\n",
        "\n",
        "# from google.colab import userdata\n",
        "# os.environ[\"OPENAI_API_KEY\"] = userdata.get('openai_api_key')\n",
        "\n",
        "#Please insert your own GPT-4 enabled API key as a string here:\n",
        "#os.environ[\"OPENAI_API_KEY\"] = \"X\"\n",
        "os.environ['CLAUDE_API_KEY'] = \"X\"\n",
        "#os.environ['GEMINI_API_KEY'] =\"X\"\n",
        "#os.environ[\"MODEL\"] = \"ChatGPT\"\n",
        "os.environ[\"MODEL\"] = \"Claude\"\n",
        "#os.environ[\"MODEL\"] = \"Gemini\""
      ],
      "metadata": {
        "id": "A4k6AgcKeABT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Mux Hierarchy Example"
      ],
      "metadata": {
        "id": "E1ME8-Hx0n23"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Submodules\n",
        "\n",
        "\n",
        "### Each step is structured as [\"filename\",\"natural language description\"]\n",
        "submodules = [\n",
        "    [\"mux2to1\",\"2-to-1 multiplexer\",\"input wire in1, input wire in2, input wire select, output wire out\"],\n",
        "    [\"mux4to1\",\"4-to-1 multiplexer\",\"input [1:0] sel, input [3:0] in, output reg out\"],\n",
        "    [\"mux8to1\",\"8-to-1 multiplexer\",\"input [2:0] sel, input [7:0] in, output reg out\"],\n",
        "]"
      ],
      "metadata": {
        "id": "HsCf-Y9zOYnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hier_gen(submodules)"
      ],
      "metadata": {
        "id": "HHVIF2lgWZJV"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
